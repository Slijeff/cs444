{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies for AI gym to run properly (shouldn't take more than a minute). If running on google cloud or running locally, only need to run once. Colab may require installing everytime the vm shuts down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /opt/conda/lib/python3.11/site-packages (0.26.2)\n",
      "Requirement already satisfied: pyvirtualdisplay in /opt/conda/lib/python3.11/site-packages (3.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.11/site-packages (from gym) (1.24.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from gym) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /opt/conda/lib/python3.11/site-packages (from gym) (0.0.8)\n",
      "sudo: The \"no new privileges\" flag is set, which prevents sudo from running as root.\n",
      "sudo: If sudo is running in a container, you may need to adjust the container configuration to disable the flag.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gym pyvirtualdisplay\n",
    "!sudo apt-get install -y xvfb python-opengl ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade setuptools --user\n",
    "!pip3 install ez_setup \n",
    "!pip3 install gym[atari] \n",
    "!pip3 install gym[accept-rom-license] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import find_max_lives, check_live, get_frame, get_init_state\n",
    "from model import DQN, DQN_LSTM\n",
    "from config import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialize our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. \n",
    "\n",
    "In breakout, we will use 3 actions \"fire\", \"left\", and \"right\". \"fire\" is only used to reset the game when a life is lost, \"left\" moves the agent left and \"right\" moves the agent right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('BoxingDeterministic-v4')\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "number_lives = find_max_lives(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 18 #fire, left, and right          noop, left and right for skiing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. Once you've created a working DQN agent, use the code in agent.py to create a double DQN agent in __agent_double.py__. Set the flag \"double_dqn\" to True to train the double DQN agent.\n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_dqn = True # set to True if using double DQN agent\n",
    "\n",
    "if double_dqn:\n",
    "    from agent_double import Agent\n",
    "else:\n",
    "    from agent import Agent\n",
    "\n",
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this training loop, we do not render the screen because it slows down training signficantly. To watch the agent play the game, run the code in next section \"Visualize Agent Performance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000 0.0001 3000\n"
     ]
    }
   ],
   "source": [
    "print(train_frame, learning_rate, EPISODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue from previous training\n",
    "# agent.policy_net = torch.load(\"./save_model/breakout_dqn_latest_3.6.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: -6.0   memory length: 1786   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -6.0\n",
      "episode: 1   score: -1.0   memory length: 3572   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -3.5\n",
      "episode: 2   score: -10.0   memory length: 5358   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -5.666666666666667\n",
      "episode: 3   score: -4.0   memory length: 7144   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -5.25\n",
      "episode: 4   score: -1.0   memory length: 8930   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -4.4\n",
      "episode: 5   score: 2.0   memory length: 10716   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -3.3333333333333335\n",
      "episode: 6   score: 1.0   memory length: 12502   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -2.7142857142857144\n",
      "episode: 7   score: -4.0   memory length: 14288   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -2.875\n",
      "episode: 8   score: -4.0   memory length: 16074   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -3.0\n",
      "episode: 9   score: 9.0   memory length: 17860   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -1.8\n",
      "episode: 10   score: -5.0   memory length: 19646   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -2.090909090909091\n",
      "episode: 11   score: 6.0   memory length: 21432   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -1.4166666666666667\n",
      "episode: 12   score: -2.0   memory length: 23218   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -1.4615384615384615\n",
      "episode: 13   score: -13.0   memory length: 25004   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -2.2857142857142856\n",
      "episode: 14   score: -4.0   memory length: 26790   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -2.4\n",
      "episode: 15   score: 2.0   memory length: 28576   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -2.125\n",
      "episode: 16   score: 6.0   memory length: 30362   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -1.6470588235294117\n",
      "episode: 17   score: 0.0   memory length: 32148   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -1.5555555555555556\n",
      "episode: 18   score: 4.0   memory length: 33934   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -1.263157894736842\n",
      "episode: 19   score: 4.0   memory length: 35720   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -1.0\n",
      "episode: 20   score: 7.0   memory length: 37506   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -0.6190476190476191\n",
      "episode: 21   score: 9.0   memory length: 39292   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -0.18181818181818182\n",
      "episode: 22   score: 2.0   memory length: 41078   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -0.08695652173913043\n",
      "episode: 23   score: -8.0   memory length: 42864   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -0.4166666666666667\n",
      "episode: 24   score: -2.0   memory length: 44650   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -0.48\n",
      "episode: 25   score: 1.0   memory length: 46436   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -0.4230769230769231\n",
      "episode: 26   score: 1.0   memory length: 48222   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -0.37037037037037035\n",
      "episode: 27   score: -1.0   memory length: 50008   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -0.39285714285714285\n",
      "episode: 28   score: 4.0   memory length: 51794   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -0.2413793103448276\n",
      "episode: 29   score: 2.0   memory length: 53580   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -0.16666666666666666\n",
      "episode: 30   score: 2.0   memory length: 55366   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -0.0967741935483871\n",
      "episode: 31   score: 7.0   memory length: 57152   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.125\n",
      "episode: 32   score: 3.0   memory length: 58938   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.21212121212121213\n",
      "episode: 33   score: 6.0   memory length: 60724   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.38235294117647056\n",
      "episode: 34   score: -3.0   memory length: 62510   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.2857142857142857\n",
      "episode: 35   score: 2.0   memory length: 64296   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.3333333333333333\n",
      "episode: 36   score: 1.0   memory length: 66082   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.35135135135135137\n",
      "episode: 37   score: -1.0   memory length: 67868   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.3157894736842105\n",
      "episode: 38   score: 4.0   memory length: 69654   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.41025641025641024\n",
      "episode: 39   score: 2.0   memory length: 71440   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.45\n",
      "episode: 40   score: 9.0   memory length: 73226   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.6585365853658537\n",
      "episode: 41   score: 5.0   memory length: 75012   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.7619047619047619\n",
      "episode: 42   score: -7.0   memory length: 76798   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.5813953488372093\n",
      "episode: 43   score: 0.0   memory length: 78584   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.5681818181818182\n",
      "episode: 44   score: -4.0   memory length: 80370   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.4666666666666667\n",
      "episode: 45   score: 1.0   memory length: 82156   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.4782608695652174\n",
      "episode: 46   score: 7.0   memory length: 83942   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.6170212765957447\n",
      "episode: 47   score: 1.0   memory length: 85728   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.625\n",
      "episode: 48   score: -5.0   memory length: 87514   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.5102040816326531\n",
      "episode: 49   score: -1.0   memory length: 89300   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.48\n",
      "episode: 50   score: -1.0   memory length: 91086   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.45098039215686275\n",
      "episode: 51   score: -4.0   memory length: 92872   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.36538461538461536\n",
      "episode: 52   score: -2.0   memory length: 94658   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.32075471698113206\n",
      "episode: 53   score: -9.0   memory length: 96444   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.14814814814814814\n",
      "episode: 54   score: -7.0   memory length: 98230   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.01818181818181818\n",
      "episode: 55   score: 5.0   memory length: 100016   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.10714285714285714\n",
      "episode: 56   score: 3.0   memory length: 101802   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.15789473684210525\n",
      "episode: 57   score: 2.0   memory length: 103588   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.1896551724137931\n",
      "episode: 58   score: 11.0   memory length: 105374   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.3728813559322034\n",
      "episode: 59   score: 2.0   memory length: 107160   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.4\n",
      "episode: 60   score: 0.0   memory length: 108946   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.39344262295081966\n",
      "episode: 61   score: 1.0   memory length: 110732   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.4032258064516129\n",
      "episode: 62   score: 0.0   memory length: 112518   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.3968253968253968\n",
      "episode: 63   score: 0.0   memory length: 114304   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.390625\n",
      "episode: 64   score: 5.0   memory length: 116090   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.46153846153846156\n",
      "episode: 65   score: -9.0   memory length: 117876   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.3181818181818182\n",
      "episode: 66   score: 3.0   memory length: 119662   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.3582089552238806\n",
      "episode: 67   score: 6.0   memory length: 121448   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.4411764705882353\n",
      "episode: 68   score: 0.0   memory length: 123234   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.43478260869565216\n",
      "episode: 69   score: -3.0   memory length: 125020   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.38571428571428573\n",
      "episode: 70   score: -4.0   memory length: 126806   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.323943661971831\n",
      "episode: 71   score: 3.0   memory length: 128592   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.3611111111111111\n",
      "episode: 72   score: 1.0   memory length: 130378   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.3698630136986301\n",
      "episode: 73   score: 14.0   memory length: 132164   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.5540540540540541\n",
      "episode: 74   score: -1.0   memory length: 133950   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.5333333333333333\n",
      "episode: 75   score: -1.0   memory length: 135736   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.5131578947368421\n",
      "episode: 76   score: 4.0   memory length: 137522   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.5584415584415584\n",
      "episode: 77   score: 3.0   memory length: 139308   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.5897435897435898\n",
      "episode: 78   score: 4.0   memory length: 141094   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.6329113924050633\n",
      "episode: 79   score: -9.0   memory length: 142880   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.5125\n",
      "episode: 80   score: 2.0   memory length: 144666   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.5308641975308642\n",
      "episode: 81   score: -3.0   memory length: 146452   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.4878048780487805\n",
      "episode: 82   score: -5.0   memory length: 148238   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.42168674698795183\n",
      "episode: 83   score: 6.0   memory length: 150024   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.4880952380952381\n",
      "episode: 84   score: 1.0   memory length: 151810   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.49411764705882355\n",
      "episode: 85   score: 0.0   memory length: 153596   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.4883720930232558\n",
      "episode: 86   score: 3.0   memory length: 155382   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.5172413793103449\n",
      "episode: 87   score: 3.0   memory length: 157168   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.5454545454545454\n",
      "episode: 88   score: -2.0   memory length: 158954   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.5168539325842697\n",
      "episode: 89   score: 0.0   memory length: 160740   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.5111111111111111\n",
      "episode: 90   score: -2.0   memory length: 162526   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.4835164835164835\n",
      "episode: 91   score: 8.0   memory length: 164312   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.5652173913043478\n",
      "episode: 92   score: 0.0   memory length: 166098   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.5591397849462365\n",
      "episode: 93   score: -17.0   memory length: 167884   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.3723404255319149\n",
      "episode: 94   score: 0.0   memory length: 169670   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.3684210526315789\n",
      "episode: 95   score: 9.0   memory length: 171456   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.4583333333333333\n",
      "episode: 96   score: 1.0   memory length: 173242   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.4639175257731959\n",
      "episode: 97   score: 5.0   memory length: 175028   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.5102040816326531\n",
      "episode: 98   score: 4.0   memory length: 176814   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.5454545454545454\n",
      "episode: 99   score: -1.0   memory length: 178600   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.53\n",
      "episode: 100   score: 0.0   memory length: 180386   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.59\n",
      "episode: 101   score: 1.0   memory length: 182172   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.61\n",
      "episode: 102   score: 2.0   memory length: 183958   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.73\n",
      "episode: 103   score: 1.0   memory length: 185744   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.78\n",
      "episode: 104   score: 0.0   memory length: 187530   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.79\n",
      "episode: 105   score: -3.0   memory length: 189316   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.74\n",
      "episode: 106   score: 2.0   memory length: 191102   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.75\n",
      "episode: 107   score: 0.0   memory length: 192888   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.79\n",
      "episode: 108   score: 2.0   memory length: 194674   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.85\n",
      "episode: 109   score: 1.0   memory length: 196460   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.77\n",
      "episode: 110   score: 0.0   memory length: 198246   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.82\n",
      "episode: 111   score: -1.0   memory length: 200032   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.75\n",
      "episode: 112   score: -4.0   memory length: 201818   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.73\n",
      "episode: 113   score: 2.0   memory length: 203604   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.88\n",
      "episode: 114   score: 1.0   memory length: 205390   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.93\n",
      "episode: 115   score: 2.0   memory length: 207176   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.93\n",
      "episode: 116   score: 1.0   memory length: 208962   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.88\n",
      "episode: 117   score: 2.0   memory length: 210748   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.9\n",
      "episode: 118   score: -4.0   memory length: 212534   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.82\n",
      "episode: 119   score: -1.0   memory length: 214320   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.77\n",
      "episode: 120   score: -1.0   memory length: 216106   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.69\n",
      "episode: 121   score: -1.0   memory length: 217892   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.59\n",
      "episode: 122   score: 3.0   memory length: 219678   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.6\n",
      "episode: 123   score: 3.0   memory length: 221464   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.71\n",
      "episode: 124   score: -1.0   memory length: 223250   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.72\n",
      "episode: 125   score: -1.0   memory length: 225036   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.7\n",
      "episode: 126   score: 0.0   memory length: 226822   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.69\n",
      "episode: 127   score: 6.0   memory length: 228608   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.76\n",
      "episode: 128   score: -2.0   memory length: 230394   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.7\n",
      "episode: 129   score: -22.0   memory length: 232180   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.46\n",
      "episode: 130   score: -4.0   memory length: 233966   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.4\n",
      "episode: 131   score: 0.0   memory length: 235752   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.33\n",
      "episode: 132   score: -1.0   memory length: 237538   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.29\n",
      "episode: 133   score: -7.0   memory length: 239324   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.16\n",
      "episode: 134   score: -3.0   memory length: 241110   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.16\n",
      "episode: 135   score: 0.0   memory length: 242896   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.14\n",
      "episode: 136   score: 0.0   memory length: 244682   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.13\n",
      "episode: 137   score: 5.0   memory length: 246468   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.19\n",
      "episode: 138   score: 0.0   memory length: 248254   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.15\n",
      "episode: 139   score: 12.0   memory length: 250040   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.25\n",
      "episode: 140   score: 4.0   memory length: 251826   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.2\n",
      "episode: 141   score: 14.0   memory length: 253612   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.29\n",
      "episode: 142   score: -5.0   memory length: 255398   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.31\n",
      "episode: 143   score: 13.0   memory length: 257184   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.44\n",
      "episode: 144   score: 3.0   memory length: 258970   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.51\n",
      "episode: 145   score: 4.0   memory length: 260756   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.54\n",
      "episode: 146   score: -11.0   memory length: 262542   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.36\n",
      "episode: 147   score: 3.0   memory length: 264328   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.38\n",
      "episode: 148   score: 11.0   memory length: 266114   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.54\n",
      "episode: 149   score: -7.0   memory length: 267900   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.48\n",
      "episode: 150   score: 0.0   memory length: 269686   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.49\n",
      "episode: 151   score: 11.0   memory length: 271472   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.64\n",
      "episode: 152   score: 4.0   memory length: 273258   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.7\n",
      "episode: 153   score: 7.0   memory length: 275044   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.86\n",
      "episode: 154   score: 1.0   memory length: 276830   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.94\n",
      "episode: 155   score: 2.0   memory length: 278616   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.91\n",
      "episode: 156   score: -6.0   memory length: 280402   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.82\n",
      "episode: 157   score: 2.0   memory length: 282188   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.82\n",
      "episode: 158   score: -1.0   memory length: 283974   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.7\n",
      "episode: 159   score: -1.0   memory length: 285760   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.67\n",
      "episode: 160   score: -1.0   memory length: 287546   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.66\n",
      "episode: 161   score: 10.0   memory length: 289332   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.75\n",
      "episode: 162   score: -3.0   memory length: 291118   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.72\n",
      "episode: 163   score: -2.0   memory length: 292904   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.7\n",
      "episode: 164   score: -2.0   memory length: 294690   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.63\n",
      "episode: 165   score: -3.0   memory length: 296476   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.69\n",
      "episode: 166   score: 0.0   memory length: 298262   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.66\n",
      "episode: 167   score: 2.0   memory length: 300048   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.62\n",
      "episode: 168   score: 3.0   memory length: 301834   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.65\n",
      "episode: 169   score: 9.0   memory length: 303620   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.77\n",
      "episode: 170   score: 0.0   memory length: 305406   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.81\n",
      "episode: 171   score: 2.0   memory length: 307192   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.8\n",
      "episode: 172   score: 1.0   memory length: 308978   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.8\n",
      "episode: 173   score: 6.0   memory length: 310764   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.72\n",
      "episode: 174   score: 4.0   memory length: 312550   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.77\n",
      "episode: 175   score: 4.0   memory length: 314336   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.82\n",
      "episode: 176   score: -2.0   memory length: 316122   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.76\n",
      "episode: 177   score: 2.0   memory length: 317908   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.75\n",
      "episode: 178   score: -5.0   memory length: 319694   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.66\n",
      "episode: 179   score: 3.0   memory length: 321480   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.78\n",
      "episode: 180   score: -18.0   memory length: 323266   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.58\n",
      "episode: 181   score: -10.0   memory length: 325052   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.51\n",
      "episode: 182   score: 2.0   memory length: 326838   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.58\n",
      "episode: 183   score: -5.0   memory length: 328624   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.47\n",
      "episode: 184   score: 7.0   memory length: 330410   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.53\n",
      "episode: 185   score: 1.0   memory length: 332196   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.54\n",
      "episode: 186   score: -2.0   memory length: 333982   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.49\n",
      "episode: 187   score: 5.0   memory length: 335768   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.51\n",
      "episode: 188   score: -16.0   memory length: 337554   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.37\n",
      "episode: 189   score: 0.0   memory length: 339340   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.37\n",
      "episode: 190   score: 0.0   memory length: 341126   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.39\n",
      "episode: 191   score: -6.0   memory length: 342912   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.25\n",
      "episode: 192   score: -1.0   memory length: 344698   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.24\n",
      "episode: 193   score: -4.0   memory length: 346484   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.37\n",
      "episode: 194   score: 10.0   memory length: 348270   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.47\n",
      "episode: 195   score: 0.0   memory length: 350056   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.38\n",
      "episode: 196   score: -1.0   memory length: 351842   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.36\n",
      "episode: 197   score: -2.0   memory length: 353628   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.29\n",
      "episode: 198   score: 2.0   memory length: 355414   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.27\n",
      "episode: 199   score: 3.0   memory length: 357200   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.31\n",
      "episode: 200   score: -18.0   memory length: 358986   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.13\n",
      "episode: 201   score: 4.0   memory length: 360772   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.16\n",
      "episode: 202   score: 1.0   memory length: 362558   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.15\n",
      "episode: 203   score: -5.0   memory length: 364344   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.09\n",
      "episode: 204   score: -14.0   memory length: 366130   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -0.05\n",
      "episode: 205   score: 6.0   memory length: 367916   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.04\n",
      "episode: 206   score: -2.0   memory length: 369702   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.0\n",
      "episode: 207   score: -1.0   memory length: 371488   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -0.01\n",
      "episode: 208   score: 0.0   memory length: 373274   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -0.03\n",
      "episode: 209   score: 6.0   memory length: 375060   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.02\n",
      "episode: 210   score: 3.0   memory length: 376846   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.05\n",
      "episode: 211   score: 3.0   memory length: 378632   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.09\n",
      "episode: 212   score: -4.0   memory length: 380418   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.09\n",
      "episode: 213   score: -11.0   memory length: 382204   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -0.04\n",
      "episode: 214   score: 0.0   memory length: 383990   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -0.05\n",
      "episode: 215   score: -1.0   memory length: 385776   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -0.08\n",
      "episode: 216   score: -2.0   memory length: 387562   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -0.11\n",
      "episode: 217   score: -3.0   memory length: 389348   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -0.16\n",
      "episode: 218   score: -1.0   memory length: 391134   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: -0.13\n",
      "episode: 219   score: 13.0   memory length: 392920   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.01\n",
      "episode: 220   score: 0.0   memory length: 394706   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.02\n",
      "episode: 221   score: 0.0   memory length: 396492   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.03\n",
      "episode: 222   score: 1.0   memory length: 398278   epsilon: 1.0    steps: 1786    lr: 0.0001     evaluation reward: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhui8/cs444/assignment5/memory.py:30: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  sample = np.array(sample, dtype=object)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 223   score: 4.0   memory length: 400064   epsilon: 0.9999356500000014    steps: 1786    lr: 0.0001     evaluation reward: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhui8/cs444/assignment5/agent_double.py:75: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  mini_batch = np.array(mini_batch, dtype=object).transpose()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 224   score: -4.0   memory length: 401850   epsilon: 0.9981675100000398    steps: 1786    lr: 0.0001     evaluation reward: -0.01\n",
      "episode: 225   score: -5.0   memory length: 403636   epsilon: 0.9963993700000782    steps: 1786    lr: 0.0001     evaluation reward: -0.05\n",
      "episode: 226   score: 10.0   memory length: 405422   epsilon: 0.9946312300001166    steps: 1786    lr: 0.0001     evaluation reward: 0.05\n",
      "episode: 227   score: -1.0   memory length: 407208   epsilon: 0.9928630900001549    steps: 1786    lr: 0.0001     evaluation reward: -0.02\n",
      "episode: 228   score: -2.0   memory length: 408994   epsilon: 0.9910949500001933    steps: 1786    lr: 0.0001     evaluation reward: -0.02\n",
      "episode: 229   score: 2.0   memory length: 410780   epsilon: 0.9893268100002317    steps: 1786    lr: 0.0001     evaluation reward: 0.22\n",
      "episode: 230   score: 4.0   memory length: 412566   epsilon: 0.9875586700002701    steps: 1786    lr: 0.0001     evaluation reward: 0.3\n",
      "episode: 231   score: 8.0   memory length: 414352   epsilon: 0.9857905300003085    steps: 1786    lr: 0.0001     evaluation reward: 0.38\n",
      "episode: 232   score: 3.0   memory length: 416138   epsilon: 0.9840223900003469    steps: 1786    lr: 0.0001     evaluation reward: 0.42\n",
      "episode: 233   score: 2.0   memory length: 417924   epsilon: 0.9822542500003852    steps: 1786    lr: 0.0001     evaluation reward: 0.51\n",
      "episode: 234   score: -3.0   memory length: 419710   epsilon: 0.9804861100004236    steps: 1786    lr: 0.0001     evaluation reward: 0.51\n",
      "episode: 235   score: -9.0   memory length: 421496   epsilon: 0.978717970000462    steps: 1786    lr: 0.0001     evaluation reward: 0.42\n",
      "episode: 236   score: 10.0   memory length: 423282   epsilon: 0.9769498300005004    steps: 1786    lr: 0.0001     evaluation reward: 0.52\n",
      "episode: 237   score: 11.0   memory length: 425068   epsilon: 0.9751816900005388    steps: 1786    lr: 0.0001     evaluation reward: 0.58\n",
      "episode: 238   score: 11.0   memory length: 426854   epsilon: 0.9734135500005772    steps: 1786    lr: 0.0001     evaluation reward: 0.69\n",
      "episode: 239   score: -5.0   memory length: 428640   epsilon: 0.9716454100006156    steps: 1786    lr: 0.0001     evaluation reward: 0.52\n",
      "episode: 240   score: 2.0   memory length: 430426   epsilon: 0.9698772700006539    steps: 1786    lr: 0.0001     evaluation reward: 0.5\n",
      "episode: 241   score: 0.0   memory length: 432212   epsilon: 0.9681091300006923    steps: 1786    lr: 0.0001     evaluation reward: 0.36\n",
      "episode: 242   score: 12.0   memory length: 433998   epsilon: 0.9663409900007307    steps: 1786    lr: 0.0001     evaluation reward: 0.53\n",
      "episode: 243   score: -2.0   memory length: 435784   epsilon: 0.9645728500007691    steps: 1786    lr: 0.0001     evaluation reward: 0.38\n",
      "episode: 244   score: 0.0   memory length: 437570   epsilon: 0.9628047100008075    steps: 1786    lr: 0.0001     evaluation reward: 0.35\n",
      "episode: 245   score: 11.0   memory length: 439356   epsilon: 0.9610365700008459    steps: 1786    lr: 0.0001     evaluation reward: 0.42\n",
      "episode: 246   score: -11.0   memory length: 441142   epsilon: 0.9592684300008842    steps: 1786    lr: 0.0001     evaluation reward: 0.42\n",
      "episode: 247   score: 6.0   memory length: 442928   epsilon: 0.9575002900009226    steps: 1786    lr: 0.0001     evaluation reward: 0.45\n",
      "episode: 248   score: -5.0   memory length: 444714   epsilon: 0.955732150000961    steps: 1786    lr: 0.0001     evaluation reward: 0.29\n",
      "episode: 249   score: -2.0   memory length: 446500   epsilon: 0.9539640100009994    steps: 1786    lr: 0.0001     evaluation reward: 0.34\n",
      "episode: 250   score: -4.0   memory length: 448286   epsilon: 0.9521958700010378    steps: 1786    lr: 0.0001     evaluation reward: 0.3\n",
      "episode: 251   score: 4.0   memory length: 450072   epsilon: 0.9504277300010762    steps: 1786    lr: 0.0001     evaluation reward: 0.23\n",
      "episode: 252   score: 2.0   memory length: 451858   epsilon: 0.9486595900011145    steps: 1786    lr: 0.0001     evaluation reward: 0.21\n",
      "episode: 253   score: -6.0   memory length: 453644   epsilon: 0.9468914500011529    steps: 1786    lr: 0.0001     evaluation reward: 0.08\n",
      "episode: 254   score: 5.0   memory length: 455430   epsilon: 0.9451233100011913    steps: 1786    lr: 0.0001     evaluation reward: 0.12\n",
      "episode: 255   score: 4.0   memory length: 457216   epsilon: 0.9433551700012297    steps: 1786    lr: 0.0001     evaluation reward: 0.14\n",
      "episode: 256   score: 5.0   memory length: 459002   epsilon: 0.9415870300012681    steps: 1786    lr: 0.0001     evaluation reward: 0.25\n",
      "episode: 257   score: 1.0   memory length: 460788   epsilon: 0.9398188900013065    steps: 1786    lr: 0.0001     evaluation reward: 0.24\n",
      "episode: 258   score: 1.0   memory length: 462574   epsilon: 0.9380507500013449    steps: 1786    lr: 0.0001     evaluation reward: 0.26\n",
      "episode: 259   score: 4.0   memory length: 464360   epsilon: 0.9362826100013832    steps: 1786    lr: 0.0001     evaluation reward: 0.31\n",
      "episode: 260   score: -4.0   memory length: 466146   epsilon: 0.9345144700014216    steps: 1786    lr: 0.0001     evaluation reward: 0.28\n",
      "episode: 261   score: 10.0   memory length: 467932   epsilon: 0.93274633000146    steps: 1786    lr: 0.0001     evaluation reward: 0.28\n",
      "episode: 262   score: -1.0   memory length: 469718   epsilon: 0.9309781900014984    steps: 1786    lr: 0.0001     evaluation reward: 0.3\n",
      "episode: 263   score: 2.0   memory length: 471504   epsilon: 0.9292100500015368    steps: 1786    lr: 0.0001     evaluation reward: 0.34\n",
      "episode: 264   score: 4.0   memory length: 473290   epsilon: 0.9274419100015752    steps: 1786    lr: 0.0001     evaluation reward: 0.4\n",
      "episode: 265   score: -3.0   memory length: 475076   epsilon: 0.9256737700016135    steps: 1786    lr: 0.0001     evaluation reward: 0.4\n",
      "episode: 266   score: -2.0   memory length: 476862   epsilon: 0.9239056300016519    steps: 1786    lr: 0.0001     evaluation reward: 0.38\n",
      "episode: 267   score: -4.0   memory length: 478648   epsilon: 0.9221374900016903    steps: 1786    lr: 0.0001     evaluation reward: 0.32\n",
      "episode: 268   score: 1.0   memory length: 480434   epsilon: 0.9203693500017287    steps: 1786    lr: 0.0001     evaluation reward: 0.3\n",
      "episode: 269   score: 0.0   memory length: 482220   epsilon: 0.9186012100017671    steps: 1786    lr: 0.0001     evaluation reward: 0.21\n",
      "episode: 270   score: 1.0   memory length: 484006   epsilon: 0.9168330700018055    steps: 1786    lr: 0.0001     evaluation reward: 0.22\n",
      "episode: 271   score: 5.0   memory length: 485792   epsilon: 0.9150649300018439    steps: 1786    lr: 0.0001     evaluation reward: 0.25\n",
      "episode: 272   score: 5.0   memory length: 487578   epsilon: 0.9132967900018822    steps: 1786    lr: 0.0001     evaluation reward: 0.29\n",
      "episode: 273   score: -1.0   memory length: 489364   epsilon: 0.9115286500019206    steps: 1786    lr: 0.0001     evaluation reward: 0.22\n",
      "episode: 274   score: -1.0   memory length: 491150   epsilon: 0.909760510001959    steps: 1786    lr: 0.0001     evaluation reward: 0.17\n",
      "episode: 275   score: 0.0   memory length: 492936   epsilon: 0.9079923700019974    steps: 1786    lr: 0.0001     evaluation reward: 0.13\n",
      "episode: 276   score: 6.0   memory length: 494722   epsilon: 0.9062242300020358    steps: 1786    lr: 0.0001     evaluation reward: 0.21\n",
      "episode: 277   score: 2.0   memory length: 496508   epsilon: 0.9044560900020742    steps: 1786    lr: 0.0001     evaluation reward: 0.21\n",
      "episode: 278   score: 2.0   memory length: 498294   epsilon: 0.9026879500021125    steps: 1786    lr: 0.0001     evaluation reward: 0.28\n",
      "episode: 279   score: -2.0   memory length: 500080   epsilon: 0.9009198100021509    steps: 1786    lr: 4e-05     evaluation reward: 0.23\n",
      "episode: 280   score: 1.0   memory length: 501866   epsilon: 0.8991516700021893    steps: 1786    lr: 4e-05     evaluation reward: 0.42\n",
      "episode: 281   score: 5.0   memory length: 503652   epsilon: 0.8973835300022277    steps: 1786    lr: 4e-05     evaluation reward: 0.57\n",
      "episode: 282   score: 4.0   memory length: 505438   epsilon: 0.8956153900022661    steps: 1786    lr: 4e-05     evaluation reward: 0.59\n",
      "episode: 283   score: -2.0   memory length: 507224   epsilon: 0.8938472500023045    steps: 1786    lr: 4e-05     evaluation reward: 0.62\n",
      "episode: 284   score: -2.0   memory length: 509010   epsilon: 0.8920791100023429    steps: 1786    lr: 4e-05     evaluation reward: 0.53\n",
      "episode: 285   score: -14.0   memory length: 510796   epsilon: 0.8903109700023812    steps: 1786    lr: 4e-05     evaluation reward: 0.38\n",
      "episode: 286   score: -3.0   memory length: 512582   epsilon: 0.8885428300024196    steps: 1786    lr: 4e-05     evaluation reward: 0.37\n",
      "episode: 287   score: 1.0   memory length: 514368   epsilon: 0.886774690002458    steps: 1786    lr: 4e-05     evaluation reward: 0.33\n",
      "episode: 288   score: -1.0   memory length: 516154   epsilon: 0.8850065500024964    steps: 1786    lr: 4e-05     evaluation reward: 0.48\n",
      "episode: 289   score: 12.0   memory length: 517940   epsilon: 0.8832384100025348    steps: 1786    lr: 4e-05     evaluation reward: 0.6\n",
      "episode: 290   score: 4.0   memory length: 519726   epsilon: 0.8814702700025732    steps: 1786    lr: 4e-05     evaluation reward: 0.64\n",
      "episode: 291   score: 5.0   memory length: 521512   epsilon: 0.8797021300026115    steps: 1786    lr: 4e-05     evaluation reward: 0.75\n",
      "episode: 292   score: 5.0   memory length: 523298   epsilon: 0.8779339900026499    steps: 1786    lr: 4e-05     evaluation reward: 0.81\n",
      "episode: 293   score: 2.0   memory length: 525084   epsilon: 0.8761658500026883    steps: 1786    lr: 4e-05     evaluation reward: 0.87\n",
      "episode: 294   score: 4.0   memory length: 526870   epsilon: 0.8743977100027267    steps: 1786    lr: 4e-05     evaluation reward: 0.81\n",
      "episode: 295   score: -3.0   memory length: 528656   epsilon: 0.8726295700027651    steps: 1786    lr: 4e-05     evaluation reward: 0.78\n",
      "episode: 296   score: -3.0   memory length: 530442   epsilon: 0.8708614300028035    steps: 1786    lr: 4e-05     evaluation reward: 0.76\n",
      "episode: 297   score: -6.0   memory length: 532228   epsilon: 0.8690932900028419    steps: 1786    lr: 4e-05     evaluation reward: 0.72\n",
      "episode: 298   score: -7.0   memory length: 534014   epsilon: 0.8673251500028802    steps: 1786    lr: 4e-05     evaluation reward: 0.63\n",
      "episode: 299   score: 3.0   memory length: 535800   epsilon: 0.8655570100029186    steps: 1786    lr: 4e-05     evaluation reward: 0.63\n",
      "episode: 300   score: 3.0   memory length: 537586   epsilon: 0.863788870002957    steps: 1786    lr: 4e-05     evaluation reward: 0.84\n",
      "episode: 301   score: 6.0   memory length: 539372   epsilon: 0.8620207300029954    steps: 1786    lr: 4e-05     evaluation reward: 0.86\n",
      "episode: 302   score: 0.0   memory length: 541158   epsilon: 0.8602525900030338    steps: 1786    lr: 4e-05     evaluation reward: 0.85\n",
      "episode: 303   score: 11.0   memory length: 542944   epsilon: 0.8584844500030722    steps: 1786    lr: 4e-05     evaluation reward: 1.01\n",
      "episode: 304   score: 1.0   memory length: 544730   epsilon: 0.8567163100031105    steps: 1786    lr: 4e-05     evaluation reward: 1.16\n",
      "episode: 305   score: -1.0   memory length: 546516   epsilon: 0.8549481700031489    steps: 1786    lr: 4e-05     evaluation reward: 1.09\n",
      "episode: 306   score: -5.0   memory length: 548302   epsilon: 0.8531800300031873    steps: 1786    lr: 4e-05     evaluation reward: 1.06\n",
      "episode: 307   score: 7.0   memory length: 550088   epsilon: 0.8514118900032257    steps: 1786    lr: 4e-05     evaluation reward: 1.14\n",
      "episode: 308   score: 0.0   memory length: 551874   epsilon: 0.8496437500032641    steps: 1786    lr: 4e-05     evaluation reward: 1.14\n",
      "episode: 309   score: 5.0   memory length: 553660   epsilon: 0.8478756100033025    steps: 1786    lr: 4e-05     evaluation reward: 1.13\n",
      "episode: 310   score: 19.0   memory length: 555446   epsilon: 0.8461074700033409    steps: 1786    lr: 4e-05     evaluation reward: 1.29\n",
      "episode: 311   score: -2.0   memory length: 557232   epsilon: 0.8443393300033792    steps: 1786    lr: 4e-05     evaluation reward: 1.24\n",
      "episode: 312   score: 1.0   memory length: 559018   epsilon: 0.8425711900034176    steps: 1786    lr: 4e-05     evaluation reward: 1.29\n",
      "episode: 313   score: 4.0   memory length: 560804   epsilon: 0.840803050003456    steps: 1786    lr: 4e-05     evaluation reward: 1.44\n",
      "episode: 314   score: 0.0   memory length: 562590   epsilon: 0.8390349100034944    steps: 1786    lr: 4e-05     evaluation reward: 1.44\n",
      "episode: 315   score: -10.0   memory length: 564376   epsilon: 0.8372667700035328    steps: 1786    lr: 4e-05     evaluation reward: 1.35\n",
      "episode: 316   score: 4.0   memory length: 566162   epsilon: 0.8354986300035712    steps: 1786    lr: 4e-05     evaluation reward: 1.41\n",
      "episode: 317   score: 2.0   memory length: 567948   epsilon: 0.8337304900036095    steps: 1786    lr: 4e-05     evaluation reward: 1.46\n",
      "episode: 318   score: 7.0   memory length: 569734   epsilon: 0.8319623500036479    steps: 1786    lr: 4e-05     evaluation reward: 1.54\n",
      "episode: 319   score: 2.0   memory length: 571520   epsilon: 0.8301942100036863    steps: 1786    lr: 4e-05     evaluation reward: 1.43\n",
      "episode: 320   score: -6.0   memory length: 573306   epsilon: 0.8284260700037247    steps: 1786    lr: 4e-05     evaluation reward: 1.37\n",
      "episode: 321   score: 16.0   memory length: 575092   epsilon: 0.8266579300037631    steps: 1786    lr: 4e-05     evaluation reward: 1.53\n",
      "episode: 322   score: 2.0   memory length: 576878   epsilon: 0.8248897900038015    steps: 1786    lr: 4e-05     evaluation reward: 1.54\n",
      "episode: 323   score: 14.0   memory length: 578664   epsilon: 0.8231216500038399    steps: 1786    lr: 4e-05     evaluation reward: 1.64\n",
      "episode: 324   score: 12.0   memory length: 580450   epsilon: 0.8213535100038782    steps: 1786    lr: 4e-05     evaluation reward: 1.8\n",
      "episode: 325   score: 4.0   memory length: 582236   epsilon: 0.8195853700039166    steps: 1786    lr: 4e-05     evaluation reward: 1.89\n",
      "episode: 326   score: 4.0   memory length: 584022   epsilon: 0.817817230003955    steps: 1786    lr: 4e-05     evaluation reward: 1.83\n",
      "episode: 327   score: -5.0   memory length: 585808   epsilon: 0.8160490900039934    steps: 1786    lr: 4e-05     evaluation reward: 1.79\n",
      "episode: 328   score: -1.0   memory length: 587594   epsilon: 0.8142809500040318    steps: 1786    lr: 4e-05     evaluation reward: 1.8\n",
      "episode: 329   score: 12.0   memory length: 589380   epsilon: 0.8125128100040702    steps: 1786    lr: 4e-05     evaluation reward: 1.9\n",
      "episode: 330   score: 8.0   memory length: 591166   epsilon: 0.8107446700041085    steps: 1786    lr: 4e-05     evaluation reward: 1.94\n",
      "episode: 331   score: 6.0   memory length: 592952   epsilon: 0.8089765300041469    steps: 1786    lr: 4e-05     evaluation reward: 1.92\n",
      "episode: 332   score: 6.0   memory length: 594738   epsilon: 0.8072083900041853    steps: 1786    lr: 4e-05     evaluation reward: 1.95\n",
      "episode: 333   score: 2.0   memory length: 596524   epsilon: 0.8054402500042237    steps: 1786    lr: 4e-05     evaluation reward: 1.95\n",
      "episode: 334   score: 0.0   memory length: 598310   epsilon: 0.8036721100042621    steps: 1786    lr: 4e-05     evaluation reward: 1.98\n",
      "episode: 335   score: -5.0   memory length: 600096   epsilon: 0.8019039700043005    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 2.02\n",
      "episode: 336   score: -3.0   memory length: 601882   epsilon: 0.8001358300043389    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 1.89\n",
      "episode: 337   score: -1.0   memory length: 603668   epsilon: 0.7983676900043772    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 1.77\n",
      "episode: 338   score: 1.0   memory length: 605454   epsilon: 0.7965995500044156    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 1.67\n",
      "episode: 339   score: 3.0   memory length: 607240   epsilon: 0.794831410004454    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 1.75\n",
      "episode: 340   score: -1.0   memory length: 609026   epsilon: 0.7930632700044924    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 1.72\n",
      "episode: 341   score: 2.0   memory length: 610812   epsilon: 0.7912951300045308    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 1.74\n",
      "episode: 342   score: -26.0   memory length: 612598   epsilon: 0.7895269900045692    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 1.36\n",
      "episode: 343   score: -2.0   memory length: 614384   epsilon: 0.7877588500046075    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 1.36\n",
      "episode: 344   score: 1.0   memory length: 616170   epsilon: 0.7859907100046459    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 1.37\n",
      "episode: 345   score: 31.0   memory length: 617956   epsilon: 0.7842225700046843    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 1.57\n",
      "episode: 346   score: 9.0   memory length: 619742   epsilon: 0.7824544300047227    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 1.77\n",
      "episode: 347   score: 2.0   memory length: 621528   epsilon: 0.7806862900047611    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 1.73\n",
      "episode: 348   score: 1.0   memory length: 623314   epsilon: 0.7789181500047995    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 1.79\n",
      "episode: 349   score: 0.0   memory length: 625100   epsilon: 0.7771500100048379    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 1.81\n",
      "episode: 350   score: 4.0   memory length: 626886   epsilon: 0.7753818700048762    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 1.89\n",
      "episode: 351   score: 3.0   memory length: 628672   epsilon: 0.7736137300049146    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 1.88\n",
      "episode: 352   score: 2.0   memory length: 630458   epsilon: 0.771845590004953    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 1.88\n",
      "episode: 353   score: 0.0   memory length: 632244   epsilon: 0.7700774500049914    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 1.94\n",
      "episode: 354   score: 5.0   memory length: 634030   epsilon: 0.7683093100050298    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 1.94\n",
      "episode: 355   score: -1.0   memory length: 635816   epsilon: 0.7665411700050682    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 1.89\n",
      "episode: 356   score: 2.0   memory length: 637602   epsilon: 0.7647730300051065    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 1.86\n",
      "episode: 357   score: 7.0   memory length: 639388   epsilon: 0.7630048900051449    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 1.92\n",
      "episode: 358   score: 7.0   memory length: 641174   epsilon: 0.7612367500051833    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 1.98\n",
      "episode: 359   score: -3.0   memory length: 642960   epsilon: 0.7594686100052217    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 1.91\n",
      "episode: 360   score: -1.0   memory length: 644746   epsilon: 0.7577004700052601    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 1.94\n",
      "episode: 361   score: -2.0   memory length: 646532   epsilon: 0.7559323300052985    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 1.82\n",
      "episode: 362   score: 5.0   memory length: 648318   epsilon: 0.7541641900053369    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 1.88\n",
      "episode: 363   score: 0.0   memory length: 650104   epsilon: 0.7523960500053752    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 1.86\n",
      "episode: 364   score: -2.0   memory length: 651890   epsilon: 0.7506279100054136    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 1.8\n",
      "episode: 365   score: 12.0   memory length: 653676   epsilon: 0.748859770005452    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 1.95\n",
      "episode: 366   score: -12.0   memory length: 655462   epsilon: 0.7470916300054904    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 1.85\n",
      "episode: 367   score: 7.0   memory length: 657248   epsilon: 0.7453234900055288    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 1.96\n",
      "episode: 368   score: 16.0   memory length: 659034   epsilon: 0.7435553500055672    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 2.11\n",
      "episode: 369   score: 0.0   memory length: 660820   epsilon: 0.7417872100056055    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 2.11\n",
      "episode: 370   score: 9.0   memory length: 662606   epsilon: 0.7400190700056439    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 2.19\n",
      "episode: 371   score: 6.0   memory length: 664392   epsilon: 0.7382509300056823    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 2.2\n",
      "episode: 372   score: -3.0   memory length: 666178   epsilon: 0.7364827900057207    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 2.12\n",
      "episode: 373   score: -2.0   memory length: 667964   epsilon: 0.7347146500057591    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 2.11\n",
      "episode: 374   score: 4.0   memory length: 669750   epsilon: 0.7329465100057975    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 2.16\n",
      "episode: 375   score: -9.0   memory length: 671536   epsilon: 0.7311783700058359    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 2.07\n",
      "episode: 376   score: 5.0   memory length: 673322   epsilon: 0.7294102300058742    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 2.06\n",
      "episode: 377   score: 10.0   memory length: 675108   epsilon: 0.7276420900059126    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 2.14\n",
      "episode: 378   score: 12.0   memory length: 676894   epsilon: 0.725873950005951    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 2.24\n",
      "episode: 379   score: 1.0   memory length: 678680   epsilon: 0.7241058100059894    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 2.27\n",
      "episode: 380   score: -5.0   memory length: 680466   epsilon: 0.7223376700060278    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 2.21\n",
      "episode: 381   score: 4.0   memory length: 682252   epsilon: 0.7205695300060662    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 2.2\n",
      "episode: 382   score: 3.0   memory length: 684038   epsilon: 0.7188013900061045    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 2.19\n",
      "episode: 383   score: 4.0   memory length: 685824   epsilon: 0.7170332500061429    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 2.25\n",
      "episode: 384   score: -5.0   memory length: 687610   epsilon: 0.7152651100061813    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 2.22\n",
      "episode: 385   score: 3.0   memory length: 689396   epsilon: 0.7134969700062197    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 2.39\n",
      "episode: 386   score: 1.0   memory length: 691182   epsilon: 0.7117288300062581    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 2.43\n",
      "episode: 387   score: 1.0   memory length: 692968   epsilon: 0.7099606900062965    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 2.43\n",
      "episode: 388   score: 11.0   memory length: 694754   epsilon: 0.7081925500063349    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 2.55\n",
      "episode: 389   score: 1.0   memory length: 696540   epsilon: 0.7064244100063732    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 2.44\n",
      "episode: 390   score: 2.0   memory length: 698326   epsilon: 0.7046562700064116    steps: 1786    lr: 1.6000000000000003e-05     evaluation reward: 2.42\n",
      "episode: 391   score: -6.0   memory length: 700112   epsilon: 0.70288813000645    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.31\n",
      "episode: 392   score: 0.0   memory length: 701898   epsilon: 0.7011199900064884    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.26\n",
      "episode: 393   score: 4.0   memory length: 703684   epsilon: 0.6993518500065268    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.28\n",
      "episode: 394   score: -3.0   memory length: 705470   epsilon: 0.6975837100065652    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.21\n",
      "episode: 395   score: 8.0   memory length: 707256   epsilon: 0.6958155700066035    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.32\n",
      "episode: 396   score: 1.0   memory length: 709042   epsilon: 0.6940474300066419    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.36\n",
      "episode: 397   score: -3.0   memory length: 710828   epsilon: 0.6922792900066803    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.39\n",
      "episode: 398   score: 4.0   memory length: 712614   epsilon: 0.6905111500067187    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.5\n",
      "episode: 399   score: 4.0   memory length: 714400   epsilon: 0.6887430100067571    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.51\n",
      "episode: 400   score: 3.0   memory length: 716186   epsilon: 0.6869748700067955    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.51\n",
      "episode: 401   score: 5.0   memory length: 717972   epsilon: 0.6852067300068339    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.5\n",
      "episode: 402   score: 6.0   memory length: 719758   epsilon: 0.6834385900068722    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.56\n",
      "episode: 403   score: 15.0   memory length: 721544   epsilon: 0.6816704500069106    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.6\n",
      "episode: 404   score: -8.0   memory length: 723330   epsilon: 0.679902310006949    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.51\n",
      "episode: 405   score: 0.0   memory length: 725116   epsilon: 0.6781341700069874    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.52\n",
      "episode: 406   score: 2.0   memory length: 726902   epsilon: 0.6763660300070258    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.59\n",
      "episode: 407   score: -2.0   memory length: 728688   epsilon: 0.6745978900070642    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.5\n",
      "episode: 408   score: 4.0   memory length: 730474   epsilon: 0.6728297500071025    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.54\n",
      "episode: 409   score: 2.0   memory length: 732260   epsilon: 0.6710616100071409    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.51\n",
      "episode: 410   score: 0.0   memory length: 734046   epsilon: 0.6692934700071793    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.32\n",
      "episode: 411   score: 1.0   memory length: 735832   epsilon: 0.6675253300072177    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.35\n",
      "episode: 412   score: 4.0   memory length: 737618   epsilon: 0.6657571900072561    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.38\n",
      "episode: 413   score: 6.0   memory length: 739404   epsilon: 0.6639890500072945    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.4\n",
      "episode: 414   score: 4.0   memory length: 741190   epsilon: 0.6622209100073329    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.44\n",
      "episode: 415   score: 2.0   memory length: 742976   epsilon: 0.6604527700073712    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.56\n",
      "episode: 416   score: -3.0   memory length: 744762   epsilon: 0.6586846300074096    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.49\n",
      "episode: 417   score: 0.0   memory length: 746548   epsilon: 0.656916490007448    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.47\n",
      "episode: 418   score: 10.0   memory length: 748334   epsilon: 0.6551483500074864    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.5\n",
      "episode: 419   score: 2.0   memory length: 750120   epsilon: 0.6533802100075248    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.5\n",
      "episode: 420   score: 3.0   memory length: 751906   epsilon: 0.6516120700075632    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.59\n",
      "episode: 421   score: 2.0   memory length: 753692   epsilon: 0.6498439300076015    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.45\n",
      "episode: 422   score: 9.0   memory length: 755478   epsilon: 0.6480757900076399    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.52\n",
      "episode: 423   score: 8.0   memory length: 757264   epsilon: 0.6463076500076783    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.46\n",
      "episode: 424   score: -3.0   memory length: 759050   epsilon: 0.6445395100077167    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.31\n",
      "episode: 425   score: 2.0   memory length: 760836   epsilon: 0.6427713700077551    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.29\n",
      "episode: 426   score: -1.0   memory length: 762622   epsilon: 0.6410032300077935    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.24\n",
      "episode: 427   score: 18.0   memory length: 764408   epsilon: 0.6392350900078319    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.47\n",
      "episode: 428   score: -7.0   memory length: 766194   epsilon: 0.6374669500078702    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.41\n",
      "episode: 429   score: -6.0   memory length: 767980   epsilon: 0.6356988100079086    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.23\n",
      "episode: 430   score: -6.0   memory length: 769766   epsilon: 0.633930670007947    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.09\n",
      "episode: 431   score: 0.0   memory length: 771552   epsilon: 0.6321625300079854    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.03\n",
      "episode: 432   score: 0.0   memory length: 773338   epsilon: 0.6303943900080238    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 1.97\n",
      "episode: 433   score: 6.0   memory length: 775124   epsilon: 0.6286262500080622    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.01\n",
      "episode: 434   score: -2.0   memory length: 776910   epsilon: 0.6268581100081005    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 1.99\n",
      "episode: 435   score: -2.0   memory length: 778696   epsilon: 0.6250899700081389    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.02\n",
      "episode: 436   score: 8.0   memory length: 780482   epsilon: 0.6233218300081773    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.13\n",
      "episode: 437   score: -3.0   memory length: 782268   epsilon: 0.6215536900082157    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.11\n",
      "episode: 438   score: 5.0   memory length: 784054   epsilon: 0.6197855500082541    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.15\n",
      "episode: 439   score: -10.0   memory length: 785840   epsilon: 0.6180174100082925    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.02\n",
      "episode: 440   score: -9.0   memory length: 787626   epsilon: 0.6162492700083309    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 1.94\n",
      "episode: 441   score: 15.0   memory length: 789412   epsilon: 0.6144811300083692    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.07\n",
      "episode: 442   score: 0.0   memory length: 791198   epsilon: 0.6127129900084076    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.33\n",
      "episode: 443   score: -3.0   memory length: 792984   epsilon: 0.610944850008446    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.32\n",
      "episode: 444   score: 0.0   memory length: 794770   epsilon: 0.6091767100084844    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 2.31\n",
      "episode: 445   score: -5.0   memory length: 796556   epsilon: 0.6074085700085228    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 1.95\n",
      "episode: 446   score: -1.0   memory length: 798342   epsilon: 0.6056404300085612    steps: 1786    lr: 6.400000000000001e-06     evaluation reward: 1.85\n",
      "episode: 447   score: -2.0   memory length: 800128   epsilon: 0.6038722900085995    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.81\n",
      "episode: 448   score: 0.0   memory length: 801914   epsilon: 0.6021041500086379    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.8\n",
      "episode: 449   score: 0.0   memory length: 803700   epsilon: 0.6003360100086763    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.8\n",
      "episode: 450   score: 5.0   memory length: 805486   epsilon: 0.5985678700087147    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.81\n",
      "episode: 451   score: 0.0   memory length: 807272   epsilon: 0.5967997300087531    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.78\n",
      "episode: 452   score: -3.0   memory length: 809058   epsilon: 0.5950315900087915    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.73\n",
      "episode: 453   score: 7.0   memory length: 810844   epsilon: 0.5932634500088299    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.8\n",
      "episode: 454   score: 6.0   memory length: 812630   epsilon: 0.5914953100088682    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.81\n",
      "episode: 455   score: 10.0   memory length: 814416   epsilon: 0.5897271700089066    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.92\n",
      "episode: 456   score: -10.0   memory length: 816202   epsilon: 0.587959030008945    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.8\n",
      "episode: 457   score: 2.0   memory length: 817988   epsilon: 0.5861908900089834    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.75\n",
      "episode: 458   score: 12.0   memory length: 819774   epsilon: 0.5844227500090218    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.8\n",
      "episode: 459   score: 4.0   memory length: 821560   epsilon: 0.5826546100090602    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.87\n",
      "episode: 460   score: -4.0   memory length: 823346   epsilon: 0.5808864700090985    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.84\n",
      "episode: 461   score: -3.0   memory length: 825132   epsilon: 0.5791183300091369    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.83\n",
      "episode: 462   score: 0.0   memory length: 826918   epsilon: 0.5773501900091753    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.78\n",
      "episode: 463   score: 2.0   memory length: 828704   epsilon: 0.5755820500092137    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.8\n",
      "episode: 464   score: 1.0   memory length: 830490   epsilon: 0.5738139100092521    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.83\n",
      "episode: 465   score: 4.0   memory length: 832276   epsilon: 0.5720457700092905    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.75\n",
      "episode: 466   score: -1.0   memory length: 834062   epsilon: 0.5702776300093289    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.86\n",
      "episode: 467   score: -1.0   memory length: 835848   epsilon: 0.5685094900093672    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.78\n",
      "episode: 468   score: -3.0   memory length: 837634   epsilon: 0.5667413500094056    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.59\n",
      "episode: 469   score: 8.0   memory length: 839420   epsilon: 0.564973210009444    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.67\n",
      "episode: 470   score: 3.0   memory length: 841206   epsilon: 0.5632050700094824    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.61\n",
      "episode: 471   score: -3.0   memory length: 842992   epsilon: 0.5614369300095208    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.52\n",
      "episode: 472   score: -13.0   memory length: 844778   epsilon: 0.5596687900095592    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.42\n",
      "episode: 473   score: -1.0   memory length: 846564   epsilon: 0.5579006500095975    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.43\n",
      "episode: 474   score: 1.0   memory length: 848350   epsilon: 0.5561325100096359    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.4\n",
      "episode: 475   score: 25.0   memory length: 850136   epsilon: 0.5543643700096743    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.74\n",
      "episode: 476   score: 7.0   memory length: 851922   epsilon: 0.5525962300097127    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.76\n",
      "episode: 477   score: -18.0   memory length: 853708   epsilon: 0.5508280900097511    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.48\n",
      "episode: 478   score: 2.0   memory length: 855494   epsilon: 0.5490599500097895    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.38\n",
      "episode: 479   score: 6.0   memory length: 857280   epsilon: 0.5472918100098279    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.43\n",
      "episode: 480   score: -7.0   memory length: 859066   epsilon: 0.5455236700098662    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.41\n",
      "episode: 481   score: 11.0   memory length: 860852   epsilon: 0.5437555300099046    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.48\n",
      "episode: 482   score: -18.0   memory length: 862638   epsilon: 0.541987390009943    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.27\n",
      "episode: 483   score: 27.0   memory length: 864424   epsilon: 0.5402192500099814    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.5\n",
      "episode: 484   score: 24.0   memory length: 866137   epsilon: 0.5385233800100182    steps: 1713    lr: 2.560000000000001e-06     evaluation reward: 1.79\n",
      "episode: 485   score: 15.0   memory length: 867923   epsilon: 0.5367552400100566    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.91\n",
      "episode: 486   score: -3.0   memory length: 869709   epsilon: 0.534987100010095    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.87\n",
      "episode: 487   score: 2.0   memory length: 871495   epsilon: 0.5332189600101334    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.88\n",
      "episode: 488   score: 6.0   memory length: 873281   epsilon: 0.5314508200101717    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.83\n",
      "episode: 489   score: 11.0   memory length: 875067   epsilon: 0.5296826800102101    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.93\n",
      "episode: 490   score: 3.0   memory length: 876853   epsilon: 0.5279145400102485    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.94\n",
      "episode: 491   score: -2.0   memory length: 878639   epsilon: 0.5261464000102869    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.98\n",
      "episode: 492   score: -16.0   memory length: 880425   epsilon: 0.5243782600103253    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.82\n",
      "episode: 493   score: 2.0   memory length: 882211   epsilon: 0.5226101200103637    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.8\n",
      "episode: 494   score: 1.0   memory length: 883997   epsilon: 0.520841980010402    steps: 1786    lr: 2.560000000000001e-06     evaluation reward: 1.84\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Start training after random sample generation\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(frame \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m train_frame): \u001b[38;5;66;03m# You can set train_frame to a lower value while testing your starts training earlier\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_policy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m double_dqn \u001b[38;5;129;01mand\u001b[39;00m frame \u001b[38;5;241m%\u001b[39m agent\u001b[38;5;241m.\u001b[39mupdate_target \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     40\u001b[0m         agent\u001b[38;5;241m.\u001b[39mupdate_targetnet()\n",
      "File \u001b[0;32m~/cs444/assignment5/agent_double.py:74\u001b[0m, in \u001b[0;36mAgent.train_policy_net\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_min:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_decay\n\u001b[0;32m---> 74\u001b[0m mini_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_mini_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m mini_batch \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(mini_batch, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose()\n\u001b[1;32m     77\u001b[0m history \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(mini_batch[\u001b[38;5;241m0\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/cs444/assignment5/memory.py:28\u001b[0m, in \u001b[0;36mReplayMemory.sample_mini_batch\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     26\u001b[0m sample \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(HISTORY_SIZE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 28\u001b[0m     sample\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory[i \u001b[38;5;241m+\u001b[39m j])\n\u001b[1;32m     30\u001b[0m sample \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(sample, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\n\u001b[1;32m     31\u001b[0m mini_batch\u001b[38;5;241m.\u001b[39mappend((np\u001b[38;5;241m.\u001b[39mstack(sample[:, \u001b[38;5;241m0\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), sample[\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m], sample[\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m], sample[\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m]))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rewards, episodes = [], []\n",
    "best_eval_reward = 0\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    state, _ = env.reset()\n",
    "    next_state = state\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state, HISTORY_SIZE)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "\n",
    "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
    "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "            action = torch.tensor([[0]]).cuda()\n",
    "        else:\n",
    "            action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "        state = next_state\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        \n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['lives'])\n",
    "\n",
    "        life = info['lives']\n",
    "        r = reward\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action.cpu(), r, terminal_state)\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame): # You can set train_frame to a lower value while testing your starts training earlier\n",
    "            agent.train_policy_net(frame)\n",
    "            if not double_dqn and frame % agent.update_target == 0:\n",
    "                agent.update_targetnet()\n",
    "            # Update the target network only for Double DQN only\n",
    "            if double_dqn and (frame % update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "            \n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.xlabel('Episodes')\n",
    "            pylab.ylabel('Rewards') \n",
    "            pylab.title('Episodes vs Reward')\n",
    "            pylab.savefig(\"./save_graph/skiing_dqn.png\") # save graph for training visualization\n",
    "            \n",
    "            # every episode, plot the play time\n",
    "            if e % 1 == 0:\n",
    "                print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                      len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                      \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
    "            ### Change this save condition to whatever you prefer ###\n",
    "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
    "                torch.save(agent.policy_net, \"./save_model/skiing_dqn.pth\")\n",
    "                best_eval_reward = np.mean(evaluation_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1287\n"
     ]
    }
   ],
   "source": [
    "print(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Agent Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BE AWARE THIS CODE BELOW MAY CRASH THE KERNEL IF YOU RUN THE SAME CELL TWICE.\n",
    "\n",
    "Please save your model before running this portion of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/breakout_dqn_8.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gym.wrappers import Monitor # If importing monitor raises issues, try using `from gym.wrappers import RecordVideo`\n",
    "from gym.wrappers import RecordVideo\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "# Displaying the game live\n",
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d %s\" % (\"Agent Playing\",step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())\n",
    "    \n",
    "# Recording the game and replaying the game afterwards\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "    env = Monitor(env, './video', force=True)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Xvfb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m display \u001b[38;5;241m=\u001b[39m \u001b[43mDisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisible\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m display\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load agent\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# agent.load_policy_net(\"./save_model/breakout_dqn.pth\")\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyvirtualdisplay/display.py:54\u001b[0m, in \u001b[0;36mDisplay.__init__\u001b[0;34m(self, backend, visible, size, color_depth, bgcolor, use_xauth, retries, extra_args, manage_global_env, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcls\u001b[39m:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown backend: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend)\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolor_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor_depth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbgcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbgcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_xauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_xauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# check_startup=check_startup,\u001b[39;49;00m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmanage_global_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanage_global_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyvirtualdisplay/xvfb.py:44\u001b[0m, in \u001b[0;36mXvfbDisplay.__init__\u001b[0;34m(self, size, color_depth, bgcolor, use_xauth, fbdir, dpi, retries, extra_args, manage_global_env)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fbdir \u001b[38;5;241m=\u001b[39m fbdir\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dpi \u001b[38;5;241m=\u001b[39m dpi\n\u001b[0;32m---> 44\u001b[0m \u001b[43mAbstractDisplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mPROGRAM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_xauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_xauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmanage_global_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanage_global_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyvirtualdisplay/abstractdisplay.py:85\u001b[0m, in \u001b[0;36mAbstractDisplay.__init__\u001b[0;34m(self, program, use_xauth, retries, extra_args, manage_global_env)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipe_wfd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retries_current \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 85\u001b[0m helptext \u001b[38;5;241m=\u001b[39m \u001b[43mget_helptext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprogram\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_displayfd \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-displayfd\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m helptext\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_displayfd:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyvirtualdisplay/util.py:13\u001b[0m, in \u001b[0;36mget_helptext\u001b[0;34m(program)\u001b[0m\n\u001b[1;32m      6\u001b[0m cmd \u001b[38;5;241m=\u001b[39m [program, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-help\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# py3.7+\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# p = subprocess.run(cmd, capture_output=True)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# stderr = p.stderr\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# py3.6 also\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m _, stderr \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mcommunicate()\n\u001b[1;32m     21\u001b[0m helptext \u001b[38;5;241m=\u001b[39m stderr\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[1;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/subprocess.py:1950\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001b[0m\n\u001b[1;32m   1948\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errno_num \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1949\u001b[0m         err_msg \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1950\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1951\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Xvfb'"
     ]
    }
   ],
   "source": [
    "display = Display(visible=0, size=(300, 200))\n",
    "display.start()\n",
    "\n",
    "# Load agent\n",
    "# agent.load_policy_net(\"./save_model/breakout_dqn.pth\")\n",
    "agent.epsilon = 0.0 # Set agent to only exploit the best action\n",
    "\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "env = wrap_env(env)\n",
    "\n",
    "done = False\n",
    "score = 0\n",
    "step = 0\n",
    "state = env.reset()\n",
    "next_state = state\n",
    "life = number_lives\n",
    "history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "get_init_state(history, state)\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    # Render breakout\n",
    "    env.render()\n",
    "#     show_state(env,step) # uncommenting this provides another way to visualize the game\n",
    "\n",
    "    step += 1\n",
    "    frame += 1\n",
    "\n",
    "    # Perform a fire action if ball is no longer on screen\n",
    "    if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "        action = 0\n",
    "    else:\n",
    "        action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "    state = next_state\n",
    "    \n",
    "    next_state, reward, done, info = env.step(action + 1)\n",
    "        \n",
    "    frame_next_state = get_frame(next_state)\n",
    "    history[4, :, :] = frame_next_state\n",
    "    terminal_state = check_live(life, info['ale.lives'])\n",
    "        \n",
    "    life = info['ale.lives']\n",
    "    r = np.clip(reward, -1, 1) \n",
    "    r = reward\n",
    "\n",
    "    # Store the transition in memory \n",
    "    agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "    # Start training after random sample generation\n",
    "    score += reward\n",
    "    \n",
    "    history[:4, :, :] = history[1:, :, :]\n",
    "env.close()\n",
    "show_video()\n",
    "display.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
